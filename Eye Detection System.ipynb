{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d5ad4a",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "!pip install opencv-python --user",
   "id": "a4bb4f737b1eaf2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print('hello')",
   "id": "c639967c5dd916eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "830bd058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T21:10:54.364480Z",
     "start_time": "2024-09-09T21:10:52.645488Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Path to the directory containing the training images\n",
    "path=r\"C:\\Users\\aksha\\OneDrive\\Desktop\\Senior Design BE 4950\\Eye-Detection-System\\Dataset\\Dataset\\Open_Eyes\"\n",
    "#Define the size of the input images\n",
    "img_size = 50\n",
    "#create an empty list to store the training data\n",
    "data_open = []\n",
    "# Loop over the images in tthe traing directory\n",
    "for img_name in os.listdir(path):\n",
    "    if img_name.endswith(\".jpg\") or img_name.endswith(\".png\"):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        #load the image and convert it into grayscale\n",
    "        img=cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        #Resize the images to the desired size\n",
    "        img = cv2.resize(img,(img_size,img_size))\n",
    "        #Normalize the pixel values to be between 0 and 1\n",
    "        img=img.astype('float32') / 255.0\n",
    "        # Add the image to the training data list\n",
    "        data_open.append(img)\n",
    "        \n",
    "path=r\"C:\\Users\\aksha\\OneDrive\\Desktop\\Senior Design BE 4950\\Eye-Detection-System\\Dataset\\Dataset\\Closed_Eyes\"\n",
    "data_closed = []\n",
    "for img_name in os.listdir(path):\n",
    "    if img_name.endswith(\".jpg\") or img_name.endswith(\".png\"):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        #load the image and convert it into grayscale\n",
    "        img=cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        #Resize the images to the desired size\n",
    "        img = cv2.resize(img,(img_size,img_size))\n",
    "        #Normalize the pixel values to be between 0 and 1\n",
    "        img=img.astype('float32') / 255.0\n",
    "        # Add the image to the training data list\n",
    "        data_closed.append(img)\n",
    "#Define the size of the input images\n",
    "# convert the training data list to a numpy array\n",
    "data_closed=np.array(data_closed)\n",
    "data_open=np.array(data_open)\n",
    "\n",
    "data_all = np.concatenate((data_open,data_closed))\n",
    "data_all = np.reshape(data_all,(data_all.shape[0],img_size,img_size,1))\n",
    "labels_all = np.concatenate([np.ones(len(data_open)),np.zeros(len(data_closed))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_all, labels_all, test_size=0.25, random_state=42)\n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T21:10:49.318779Z",
     "start_time": "2024-09-09T21:10:49.311907Z"
    }
   },
   "cell_type": "code",
   "source": "data_open",
   "id": "c7420b625c7f5c11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.14901961, 0.17254902, 0.1882353 , ..., 0.34901962,\n",
       "         0.3529412 , 0.3529412 ],\n",
       "        [0.14901961, 0.17254902, 0.18431373, ..., 0.36078432,\n",
       "         0.35686275, 0.3529412 ],\n",
       "        [0.14901961, 0.17254902, 0.1882353 , ..., 0.36862746,\n",
       "         0.36862746, 0.36078432],\n",
       "        ...,\n",
       "        [0.19607843, 0.20784314, 0.23137255, ..., 0.27450982,\n",
       "         0.28235295, 0.2901961 ],\n",
       "        [0.1882353 , 0.20784314, 0.23137255, ..., 0.27450982,\n",
       "         0.2784314 , 0.28627452],\n",
       "        [0.19215687, 0.2       , 0.23137255, ..., 0.27058825,\n",
       "         0.27450982, 0.2784314 ]],\n",
       "\n",
       "       [[0.3372549 , 0.34509805, 0.34117648, ..., 0.1882353 ,\n",
       "         0.1882353 , 0.1764706 ],\n",
       "        [0.32941177, 0.3372549 , 0.3372549 , ..., 0.18431373,\n",
       "         0.18431373, 0.18039216],\n",
       "        [0.3372549 , 0.33333334, 0.3372549 , ..., 0.1764706 ,\n",
       "         0.18431373, 0.1764706 ],\n",
       "        ...,\n",
       "        [0.26666668, 0.25882354, 0.25490198, ..., 0.25490198,\n",
       "         0.24313726, 0.24313726],\n",
       "        [0.25882354, 0.2509804 , 0.23921569, ..., 0.24313726,\n",
       "         0.24313726, 0.23529412],\n",
       "        [0.2627451 , 0.25490198, 0.25490198, ..., 0.24313726,\n",
       "         0.24313726, 0.22745098]],\n",
       "\n",
       "       [[0.15294118, 0.1764706 , 0.1882353 , ..., 0.34901962,\n",
       "         0.34509805, 0.35686275],\n",
       "        [0.16078432, 0.1764706 , 0.18431373, ..., 0.36078432,\n",
       "         0.3647059 , 0.36078432],\n",
       "        [0.14901961, 0.16862746, 0.18431373, ..., 0.36862746,\n",
       "         0.36862746, 0.36078432],\n",
       "        ...,\n",
       "        [0.2       , 0.21176471, 0.23137255, ..., 0.27450982,\n",
       "         0.28235295, 0.28627452],\n",
       "        [0.2       , 0.21568628, 0.23137255, ..., 0.27058825,\n",
       "         0.28627452, 0.2901961 ],\n",
       "        [0.19607843, 0.20784314, 0.23921569, ..., 0.27450982,\n",
       "         0.28235295, 0.28627452]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.19607843, 0.20784314, 0.21568628, ..., 0.41960785,\n",
       "         0.41568628, 0.43137255],\n",
       "        [0.19607843, 0.20784314, 0.22352941, ..., 0.4117647 ,\n",
       "         0.40392157, 0.42745098],\n",
       "        [0.19607843, 0.20784314, 0.22352941, ..., 0.42352942,\n",
       "         0.40392157, 0.4117647 ],\n",
       "        ...,\n",
       "        [0.20392157, 0.22352941, 0.22745098, ..., 0.32156864,\n",
       "         0.32156864, 0.3137255 ],\n",
       "        [0.20392157, 0.21960784, 0.22745098, ..., 0.3254902 ,\n",
       "         0.32156864, 0.3254902 ],\n",
       "        [0.20392157, 0.21568628, 0.22352941, ..., 0.32156864,\n",
       "         0.32156864, 0.32156864]],\n",
       "\n",
       "       [[0.35686275, 0.35686275, 0.35686275, ..., 0.1254902 ,\n",
       "         0.11372549, 0.10196079],\n",
       "        [0.35686275, 0.35686275, 0.36078432, ..., 0.12156863,\n",
       "         0.11372549, 0.10196079],\n",
       "        [0.36078432, 0.36862746, 0.36078432, ..., 0.12156863,\n",
       "         0.11372549, 0.10196079],\n",
       "        ...,\n",
       "        [0.2509804 , 0.25882354, 0.2627451 , ..., 0.1882353 ,\n",
       "         0.1764706 , 0.16862746],\n",
       "        [0.25490198, 0.25490198, 0.25490198, ..., 0.18039216,\n",
       "         0.16862746, 0.16470589],\n",
       "        [0.2509804 , 0.2509804 , 0.2509804 , ..., 0.18039216,\n",
       "         0.16862746, 0.15686275]],\n",
       "\n",
       "       [[0.19215687, 0.2       , 0.21960784, ..., 0.41960785,\n",
       "         0.41568628, 0.43529412],\n",
       "        [0.19607843, 0.20784314, 0.22352941, ..., 0.41568628,\n",
       "         0.42352942, 0.4392157 ],\n",
       "        [0.19607843, 0.20392157, 0.22745098, ..., 0.41960785,\n",
       "         0.42745098, 0.43529412],\n",
       "        ...,\n",
       "        [0.20784314, 0.22745098, 0.23529412, ..., 0.3137255 ,\n",
       "         0.31764707, 0.31764707],\n",
       "        [0.20392157, 0.21960784, 0.23921569, ..., 0.3254902 ,\n",
       "         0.31764707, 0.31764707],\n",
       "        [0.20392157, 0.21568628, 0.23137255, ..., 0.33333334,\n",
       "         0.32156864, 0.3254902 ]]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "id": "dce8a787",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e3364be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T20:49:24.519343Z",
     "start_time": "2024-09-09T20:49:23.868070Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "# Path to the directory containing the testing images\n",
    "path=r\"C:\\Users\\aksha\\OneDrive\\Desktop\\Senior Design BE 4950\\Eye-Detection-System\\Dataset\\Dataset\\test_closed\"\n",
    "#Define the size of the input images\n",
    "img_size = 50\n",
    "#create an empty list to store the testing data\n",
    "test_data = []\n",
    "# Loop over the images in tthe testing directory\n",
    "for img_name in os.listdir(path):\n",
    "    if img_name.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        #load the image and convert it into grayscale\n",
    "        img=cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        #Resize the images to the desired size\n",
    "        img = cv2.resize(img,(img_size,img_size))\n",
    "        #Normalize the pixel values to be between 0 and 1\n",
    "        img=img.astype('float32') / 255.0\n",
    "        # Add the image to the testing data list\n",
    "        test_data.append(img)\n",
    "        \n",
    "\n",
    "\n",
    "# convert the testing data list to a numpy array\n",
    "test_data=np.concatenate([test_open,test_data])\n",
    "#Reshape the testing data array to have a channel dimension\n",
    "test_data=np.reshape(test_data,(test_data.shape[0],img_size,img_size,1))\n",
    "#Save the testing data array to a file\n",
    "np.save('test_data.npy',test_data)"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "705db53e",
   "metadata": {},
   "source": [
    "# Training data labels"
   ]
  },
  {
   "cell_type": "code",
   "id": "d232c1ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T20:49:30.912361Z",
     "start_time": "2024-09-09T20:49:30.907734Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "# defibe the number of training images \n",
    "num_train_images=600\n",
    "#define the labels for the training images \n",
    "\n",
    "train_labels=np.concatenate((np.ones(num_train_images//2),np.zeros(num_train_images//2)))\n",
    "# save the training labels array to a file\n",
    "np.save('train_labels.npy',train_labels)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "f65f4c00",
   "metadata": {},
   "source": [
    "# Testing data labels"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbffe7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T20:49:34.621962Z",
     "start_time": "2024-09-09T20:49:34.616315Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "# defibe the number of training images \n",
    "num_test_images=3400\n",
    "#define the labels for the training images \n",
    "\n",
    "test_labels=np.concatenate((np.ones(num_test_images//2),np.zeros(num_test_images//2)))\n",
    "# save the training labels array to a file\n",
    "np.save('test_labels.npy',test_labels)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "90375c2c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "fc65b8dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T21:11:22.327683Z",
     "start_time": "2024-09-09T21:10:59.784248Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the data\n",
    "train_data = np.load('train_data.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "test_data = np.load('test_data.npy')\n",
    "test_labels = np.load('test_labels.npy')\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "# Save the model to a file\n",
    "model.save('eye_model.keras')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\OneDrive\\Desktop\\Senior Design BE 4950\\Eye-Detection-System\\environment\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 40ms/step - accuracy: 0.5490 - loss: 0.6739\n",
      "Epoch 2/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 40ms/step - accuracy: 0.8167 - loss: 0.5494\n",
      "Epoch 3/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 41ms/step - accuracy: 0.8852 - loss: 0.3308\n",
      "Epoch 4/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 43ms/step - accuracy: 0.9259 - loss: 0.2221\n",
      "Epoch 5/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 47ms/step - accuracy: 0.9497 - loss: 0.1653\n",
      "Epoch 6/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 44ms/step - accuracy: 0.9554 - loss: 0.1373\n",
      "Epoch 7/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 41ms/step - accuracy: 0.9659 - loss: 0.0975\n",
      "Epoch 8/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 43ms/step - accuracy: 0.9765 - loss: 0.0753\n",
      "Epoch 9/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 44ms/step - accuracy: 0.9732 - loss: 0.0743\n",
      "Epoch 10/10\n",
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 42ms/step - accuracy: 0.9779 - loss: 0.0597\n",
      "\u001B[1m32/32\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9812 - loss: 0.0564\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "train_data",
   "id": "9cd46a0b7ac2717c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01fff32b",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "id": "93d4f2a0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Load the model\n",
    "model = keras.models.load_model('eye_model.keras')\n",
    "\n",
    "# Initialize the webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the image size and the region of interest (ROI) size\n",
    "img_size = 50\n",
    "roi_size = 100  # Size of the ROI (the middle portion of the screen)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Define the coordinates for the ROI in the center of the frame\n",
    "    center_x, center_y = width // 2, height // 2\n",
    "    roi_x1 = center_x - roi_size // 2\n",
    "    roi_x2 = center_x + roi_size // 2\n",
    "    roi_y1 = center_y - roi_size // 2\n",
    "    roi_y2 = center_y + roi_size // 2\n",
    "\n",
    "    # Extract the ROI from the center of the frame\n",
    "    roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "    # Resize the ROI to the model's input size (50x50)\n",
    "    img = cv2.resize(roi, (img_size, img_size))\n",
    "\n",
    "    # Convert to grayscale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Normalize the image\n",
    "    img = img.astype('float32') / 255.0\n",
    "\n",
    "    # Expand dimensions to match the model input (batch_size, height, width, channels)\n",
    "    img = np.expand_dims(img, axis=-1)  # Add channel dimension for grayscale\n",
    "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
    "\n",
    "    # Make a prediction\n",
    "    pred = model.predict(img)\n",
    "\n",
    "    # Display the prediction result\n",
    "    if pred > 0.5:\n",
    "        eye_status = 'Eye is open'\n",
    "        color = (0, 255, 0)  # Green for open eye\n",
    "    else:\n",
    "        eye_status = 'Eye is closed'\n",
    "        color = (0, 0, 255)  # Red for closed eye\n",
    "\n",
    "    # Draw a rectangle around the ROI\n",
    "    cv2.rectangle(frame, (roi_x1, roi_y1), (roi_x2, roi_y2), color, 2)\n",
    "\n",
    "    # Display the result text on the frame\n",
    "    cv2.putText(frame, eye_status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Show the video feed with the ROI and the prediction\n",
    "    cv2.imshow('Eye Blink Detection', frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T21:18:56.461137Z",
     "start_time": "2024-09-09T21:18:34.034772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cv2.imshow('beans',img)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "c7fb22ad05518526",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
